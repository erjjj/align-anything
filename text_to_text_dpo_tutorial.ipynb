{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用Align-Anything框架进行Text-to-Text DPO训练\n",
    "\n",
    "完成LLM基础与对齐课**作业2**Bonus: 制作一个text-to-text DPO的ipynb文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备工作\n",
    "\n",
    "- Align-Anything成功部署\n",
    "- 一个偏好数据集，这里使用同作业1作业2的数据集\n",
    "- 预训练语言模型，如Qwen2.5-0.5B-Instruct\n",
    "- 2*RTX 4090"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from align_anything.models.pretrained_model import load_pretrained_models\n",
    "from align_anything.datasets.text_to_text.preference import PreferenceDataset\n",
    "from align_anything.configs.template import ChatTemplate\n",
    "from align_anything.utils.multi_process import get_current_device\n",
    "from align_anything.utils.tools import gather_log_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载预训练模型\n",
    "\n",
    "我们加载策略模型和参考模型。在DPO中，参考模型保持不变，策略模型会被优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "/home/admin01/Shizexi/align-anything/align_anything/models/pretrained_model.py:309: RuntimeWarning: The tokenizer vocabulary size (151665) is different from the model embedding size (151936) before resizing.\n",
      "  resize_tokenizer_embedding(tokenizer=tokenizer, model=model)\n",
      "/home/admin01/Shizexi/align-anything/align_anything/models/pretrained_model.py:309: RuntimeWarning: The tokenizer vocabulary size (151665) is different from the model embedding size (151936) after resizing.\n",
      "  resize_tokenizer_embedding(tokenizer=tokenizer, model=model)\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"scripts/qwen2_5/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# 加载策略模型\n",
    "policy_model, tokenizer, processor = load_pretrained_models(\n",
    "    model_name_or_path,\n",
    "    model_max_length=2048,\n",
    "    padding_side='left',\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# 加载参考模型，即策略模型训练前的状态（与策略模型相同的初始化）\n",
    "reference_model, _, _ = load_pretrained_models(\n",
    "    model_name_or_path,\n",
    "    model_max_length=2048,\n",
    "    padding_side='left',\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# 将模型移动到GPU\n",
    "device = get_current_device()\n",
    "policy_model = policy_model.to(device)\n",
    "reference_model = reference_model.to(device)\n",
    "\n",
    "# 参考模型设置为评估模式，不需要梯度\n",
    "reference_model.eval()\n",
    "for param in reference_model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(policy_model.parameters(), lr=1e-6, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置Align-Anything特有的Chat Template机制 \n",
    "使用自己写的HOMEWORK模板来格式化偏好数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_template = ChatTemplate(\n",
    "    formatter=processor,\n",
    "    template=\"HOMEWORK\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建偏好数据集\n",
    "\n",
    "使用PreferDataset类创建偏好数据集，选取train数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering valid indices: 100%|██████████| 1000/1000 [00:00<00:00, 3496.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集大小: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 创建偏好数据集\n",
    "train_dataset = PreferenceDataset(\n",
    "    path='assets/text_to_text/hw',\n",
    "    template=train_template,\n",
    "    tokenizer=tokenizer,\n",
    "    processor=processor,\n",
    "    split='train',\n",
    "    size=1000  # 使用所有3个样本\n",
    ")\n",
    "\n",
    "print(f\"数据集大小: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    collate_fn=train_dataset.get_collator(),\n",
    "    sampler=RandomSampler(train_dataset),\n",
    "    batch_size=1,  # 小批次大小\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算对数概率和损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_pad(seq: torch.Tensor, pad_token_id: int):\n",
    "    \"\"\"移除序列中的填充token\"\"\"\n",
    "    return seq[seq != pad_token_id]\n",
    "\n",
    "def compute_log_probs(model, batch, tokenizer):\n",
    "    \"\"\"计算给定序列的对数概率\"\"\"\n",
    "    # 准备输入（移除meta_info）\n",
    "    infer_batch = {k: v for k, v in batch.items() if k != 'meta_info'}\n",
    "    \n",
    "    # 计算logits\n",
    "    with torch.no_grad() if model == reference_model else torch.enable_grad():\n",
    "        logits = model(**infer_batch).logits\n",
    "    \n",
    "    device = logits.device\n",
    "    input_ids = batch['input_ids']\n",
    "    batch_size = len(batch['meta_info']['response_lens'])\n",
    "    \n",
    "    logprob_list = []\n",
    "    for idx in range(batch_size):\n",
    "        response_length = batch['meta_info']['response_lens'][idx]\n",
    "        raw_input_id = strip_pad(input_ids[idx], tokenizer.pad_token_id)\n",
    "        logit = logits[idx][-response_length:].unsqueeze(0)\n",
    "        input_id = raw_input_id[-response_length:].unsqueeze(0)\n",
    "        log_p = gather_log_probabilities(logit[:, :-1], input_id[:, 1:])\n",
    "        logprob_list.append(log_p.squeeze(0))\n",
    "    \n",
    "    return torch.nn.utils.rnn.pad_sequence(\n",
    "        logprob_list, batch_first=True, padding_value=0.0\n",
    "    ).to(device)\n",
    "\n",
    "def dpo_loss(policy_model, reference_model, batch, tokenizer, scale_coeff=1.0):\n",
    "    \"\"\"DPO损失函数\"\"\"\n",
    "    # 使用策略模型计算序列的对数概率\n",
    "    sequence_log_probs = compute_log_probs(policy_model, batch, tokenizer)\n",
    "    \n",
    "    # 将序列分成优选和劣选两部分\n",
    "    better_sequence_log_probs, worse_sequence_log_probs = sequence_log_probs.chunk(chunks=2, dim=0)\n",
    "    \n",
    "    # 使用参考模型计算序列的对数概率\n",
    "    with torch.no_grad():\n",
    "        ref_sequence_log_probs = compute_log_probs(reference_model, batch, tokenizer)\n",
    "        ref_better_sequence_log_probs, ref_worse_sequence_log_probs = ref_sequence_log_probs.chunk(chunks=2, dim=0)\n",
    "    \n",
    "    losses = []\n",
    "    better_sample_rewards = []\n",
    "    worse_sample_rewards = []\n",
    "    \n",
    "    batch_size = better_sequence_log_probs.size(0)\n",
    "    for i in range(batch_size):\n",
    "        # 计算每个序列的总对数概率\n",
    "        better_log_prob = better_sequence_log_probs[i, :].sum(dim=-1)\n",
    "        worse_log_prob = worse_sequence_log_probs[i, :].sum(dim=-1)\n",
    "        ref_better_log_prob = ref_better_sequence_log_probs[i, :].sum(dim=-1)\n",
    "        ref_worse_log_prob = ref_worse_sequence_log_probs[i, :].sum(dim=-1)\n",
    "        \n",
    "        # 计算策略模型和参考模型之间的对数比率\n",
    "        better_log_ratio = better_log_prob - ref_better_log_prob\n",
    "        worse_log_ratio = worse_log_prob - ref_worse_log_prob\n",
    "        \n",
    "        # 计算DPO损失\n",
    "        losses.append(\n",
    "            -F.logsigmoid(scale_coeff * (better_log_ratio - worse_log_ratio))\n",
    "        )\n",
    "        \n",
    "        # 记录奖励值用于监控\n",
    "        better_sample_rewards.append(scale_coeff * better_log_ratio.detach())\n",
    "        worse_sample_rewards.append(scale_coeff * worse_log_ratio.detach())\n",
    "    \n",
    "    # 计算批次的平均损失\n",
    "    loss = torch.stack(losses).mean()\n",
    "    better_sample_reward = torch.stack(better_sample_rewards)\n",
    "    worse_sample_reward = torch.stack(worse_sample_rewards)\n",
    "    reward_accuracy = (better_sample_reward > worse_sample_reward).float().mean()\n",
    "    reward_margin = (better_sample_reward - worse_sample_reward).mean()\n",
    "    \n",
    "    return {\n",
    "        'loss': loss,\n",
    "        'better_sample_reward': better_sample_reward.mean(),\n",
    "        'worse_sample_reward': worse_sample_reward.mean(),\n",
    "        'reward_accuracy': reward_accuracy,\n",
    "        'reward_margin': reward_margin,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 执行DPO训练，监控损失和奖励指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DPO训练进度:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始DPO训练...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DPO训练进度: 100%|██████████| 1000/1000 [02:34<00:00,  7.68it/s, loss=0.7786, acc=0.410, margin=0.101]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1:\n",
      "  平均损失: 0.7609\n",
      "  奖励准确率: 0.344\n",
      "  奖励边际: 0.073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DPO训练进度: 100%|██████████| 1000/1000 [02:42<00:00,  6.15it/s, loss=0.7786, acc=0.410, margin=0.101]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DPO训练完成\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 训练参数\n",
    "epochs = 1\n",
    "scale_coeff = 1.0  # DPO缩放系数\n",
    "\n",
    "# 训练监控\n",
    "progress_bar = tqdm(range(epochs * len(train_dataloader)), desc=\"DPO训练进度\")\n",
    "losses = deque(maxlen=100)\n",
    "reward_accuracies = deque(maxlen=100)\n",
    "reward_margins = deque(maxlen=100)\n",
    "\n",
    "# 创建输出目录\n",
    "os.makedirs('./output', exist_ok=True)\n",
    "\n",
    "print(\"开始DPO训练...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    policy_model.train()\n",
    "    epoch_losses = []\n",
    "    epoch_accuracies = []\n",
    "    epoch_margins = []\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        # 计算DPO损失\n",
    "        loss_dict = dpo_loss(policy_model, reference_model, batch, tokenizer, scale_coeff)\n",
    "        loss = loss_dict['loss']\n",
    "        \n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 记录指标\n",
    "        loss_value = loss.item()\n",
    "        accuracy_value = loss_dict['reward_accuracy'].item()\n",
    "        margin_value = loss_dict['reward_margin'].item()\n",
    "        \n",
    "        losses.append(loss_value)\n",
    "        reward_accuracies.append(accuracy_value)\n",
    "        reward_margins.append(margin_value)\n",
    "        \n",
    "        epoch_losses.append(loss_value)\n",
    "        epoch_accuracies.append(accuracy_value)\n",
    "        epoch_margins.append(margin_value)\n",
    "        \n",
    "        # 更新进度条\n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f\"{np.mean(losses):.4f}\",\n",
    "            'acc': f\"{np.mean(reward_accuracies):.3f}\",\n",
    "            'margin': f\"{np.mean(reward_margins):.3f}\"\n",
    "        })\n",
    "    \n",
    "    # 打印\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}:\")\n",
    "    print(f\"  平均损失: {np.mean(epoch_losses):.4f}\")\n",
    "    print(f\"  奖励准确率: {np.mean(epoch_accuracies):.3f}\")\n",
    "    print(f\"  奖励边际: {np.mean(epoch_margins):.3f}\")\n",
    "    \n",
    "# 保存模型\n",
    "policy_model.save_pretrained(f'./output/epoch_{epochs+1}')\n",
    "tokenizer.save_pretrained(f'./output/epoch_{epochs+1}')\n",
    "    \n",
    "progress_bar.close()\n",
    "print(\"\\nDPO训练完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用训练后的模型生成回答，并与参考模型（原模型）生成结果对比，验证训练效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对比训练后的模型与训练前模型的对话区别:\n",
      "\n",
      "prompt 1: 请解释什么是机器学习？\n",
      "policy response: 机器学习是一种人工智能技术，它使计算机能够从数据中自动地学习和改进。简单来说，机器学习就是让计算机通过观察大量数据来识别模式、规律，并利用这些模式进行预测或决策。\n",
      "\n",
      "在机器学习过程中，我们通常会使用算法和技术来帮助计算机学会如何从数据中提取有用的信息。这些算法可以是监督学习、无监督学习或强化学习等，它们的目标都是为了让计算机能够在没有明确编程的情况下完成特定任务。\n",
      "\n",
      "机器学习的应用\n",
      "reference response: 机器学习是一种人工智能的分支，它涉及计算机系统通过数据和算法来自动改进性能的过程。简单来说，机器学习就是让计算机从数据中学习并自我调整，以实现特定任务或决策。\n",
      "\n",
      "在机器学习中，模型（通常是一个复杂的数学公式）被训练来识别模式和规律，并且能够根据新的输入数据做出预测或决策。这个过程类似于人类学习如何解决问题，但使用的是计算机来处理大量数据。\n",
      "\n",
      "机器学习可以应用于许多领域，\n",
      "\n",
      "prompt 2: 马克，扎克伯格住在哪儿？\n",
      "policy response: 很抱歉，作为一个人工智能助手，我无法获取或提供个人的详细地址信息。我的知识和能力仅限于生成文字回答问题，并且没有访问个人信息的能力。如果您需要了解马克或者扎克伯格的具体位置信息，请咨询相关政府部门或使用其他合法途径获取准确信息。\n",
      "reference response: 对不起，我无法回答这个问题。这可能是一个涉政问题，我不会对政治人物发表评论或意见。如果您有其他想要了解的问题，请告诉我。\n"
     ]
    }
   ],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_length=100):\n",
    "    \"\"\"使用模型生成回答\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 格式化输入\n",
    "    messages = [{'role': 'user', 'content': prompt}]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    # 编码\n",
    "    inputs = tokenizer(text, return_tensors='pt').to(device)\n",
    "    \n",
    "    # 生成\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "# 测试问题\n",
    "test_prompts = [\n",
    "    \"请解释什么是机器学习？\",\n",
    "    \"马克，扎克伯格住在哪儿？\"\n",
    "]\n",
    "\n",
    "print(\"对比训练后的模型与训练前模型的对话区别:\")\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f\"\\nprompt {i+1}: {prompt}\")\n",
    "    response_policy = generate_response(policy_model, tokenizer, prompt)\n",
    "    response_reference = generate_response(reference_model, tokenizer, prompt)\n",
    "    print(f\"policy response: {response_policy}\")\n",
    "    print(f\"reference response: {response_reference}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "align-anything",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
